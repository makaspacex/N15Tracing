{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "(ODE_Lotka_Volterra_fit_multiple_ways)= \n",
    "# ODE Lotka-Volterra With Bayesian Inference in Multiple Ways\n",
    "\n",
    ":::{post} January 16, 2023\n",
    ":tags: ODE, PyTensor, gradient-free inference\n",
    ":category: intermediate, how-to\n",
    ":author: Greg Brunkhorst\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "\n",
    "from numba import njit\n",
    "from pymc.ode import DifferentialEquation\n",
    "from pytensor.compile.ops import as_op\n",
    "from scipy.integrate import odeint\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "print(f\"Running on PyMC v{pm.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext watermark\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "rng = np.random.default_rng(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "The purpose of this notebook is to demonstrate how to perform Bayesian inference on a system of ordinary differential equations (ODEs), both with and without gradients.  The accuracy and efficiency of different samplers are compared.\n",
    "\n",
    "We will first present the Lotka-Volterra predator-prey ODE model and example data.  Next, we will solve the ODE using `scipy.odeint` and (non-Bayesian) least squares optimization.  Next, we perform Bayesian inference in PyMC using non-gradient-based samplers.  Finally, we use gradient-based samplers and compare results.    \n",
    "\n",
    "### Key Conclusions\n",
    "Based on the experiments in this notebook, the most simple and efficient method for performing Bayesian inference on the Lotka-Volterra equations was to specify the ODE system in Scipy, wrap the function as a Pytensor op, and use a Differential Evolution Metropolis (DEMetropolis) sampler in PyMC.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Background\n",
    "### Motivation\n",
    "Ordinary differential equation models (ODEs) are used in a variety of science and engineering domains to model the time evolution of physical variables.  A natural choice to estimate the values and uncertainty of model parameters given experimental data is Bayesian inference.  However, ODEs can be challenging to specify and solve in the Bayesian setting, therefore, this notebook steps through multiple methods for solving an ODE inference problem using PyMC. The Lotka-Volterra model used in this example has often been used for benchmarking Bayesian inference methods (e.g., in this Stan [case study](https://mc-stan.org/users/documentation/case-studies/lotka-volterra-predator-prey.html), and in Chapter 16 of *Statistical Rethinking* {cite:p}`mcelreath2018statistical`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Lotka-Volterra Predator-Prey Model\n",
    "The Lotka-Volterra model describes the interaction between a predator and prey species. This ODE given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{d x}{dt} &=\\alpha x -\\beta xy \\\\ \n",
    "\\frac{d y}{dt} &=-\\gamma y + \\delta xy\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The state vector $X(t)=[x(t),y(t)]$ comprises the densities of the prey and the predator species respectively.  Parameters $\\boldsymbol{\\theta}=[\\alpha,\\beta,\\gamma,\\delta, x(0),y(0)]$ are the unknowns that we wish to infer from experimental observations.  $x(0), y(0)$ are the initial values of the states needed to solve the ODE, and $\\alpha,\\beta,\\gamma$, and $\\delta$ are unknown model parameters which represent the following:  \n",
    "* $\\alpha$ is the growing rate of prey when there's no predator.\n",
    "* $\\beta$ is the dying rate of prey due to predation.\n",
    "* $\\gamma$ is the dying rate of predator when there is no prey.\n",
    "* $\\delta$ is the growing rate of predator in the presence of prey.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The Hudson's Bay Company data\n",
    "The Lotka-Volterra predator prey model has been used to successfully explain the dynamics of natural populations of predators and prey, such as the lynx and snowshoe hare data of the Hudson's Bay Company. Since the dataset is small, we will hand-enter the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "data = pd.DataFrame(dict(\n",
    "    year = np.arange(1900., 1921., 1),\n",
    "    lynx = np.array([4.0, 6.1, 9.8, 35.2, 59.4, 41.7, 19.0, 13.0, 8.3, 9.1, 7.4,\n",
    "                8.0, 12.3, 19.5, 45.7, 51.1, 29.7, 15.8, 9.7, 10.1, 8.6]),\n",
    "    hare = np.array([30.0, 47.2, 70.2, 77.4, 36.3, 20.6, 18.1, 21.4, 22.0, 25.4, \n",
    "                 27.1, 40.3, 57.0, 76.6, 52.3, 19.5, 11.2, 7.6, 14.6, 16.2, 24.7])))\n",
    "data.head()\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data function for reuse later\n",
    "def plot_data(ax, lw=2, title=\"Hudson's Bay Company Data\"):\n",
    "    ax.plot(data.year, data.lynx, color=\"b\", lw=lw, marker=\"o\", markersize=12, label=\"Lynx (Data)\")\n",
    "    ax.plot(data.year, data.hare, color=\"g\", lw=lw, marker=\"+\", markersize=14, label=\"Hare (Data)\")\n",
    "    ax.legend(fontsize=14, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    ax.set_xlim([1900, 1920])\n",
    "    ax.set_ylim(0)\n",
    "    ax.set_xlabel(\"Year\", fontsize=14)\n",
    "    ax.set_ylabel(\"Pelts (Thousands)\", fontsize=14)\n",
    "    ax.set_xticks(data.year.astype(int))\n",
    "    ax.set_xticklabels(ax.get_xticks(), rotation=45)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(12, 4))\n",
    "plot_data(ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "The purpose of this analysis is to estimate, with uncertainty, the parameters for the Lotka-Volterra model for the Hudson's Bay Company data from 1900 to 1920.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scipy `odeint`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we make a Python function that represents the right-hand-side of the ODE equations with the call signature needed for the `odeint` function.  Note that Scipy's `solve_ivp` could also be used, but the older `odeint` function was faster in speed tests and is therefore used in this notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the right hand side of the ODE equations in the Scipy odeint signature\n",
    "from numba import njit\n",
    "\n",
    "\n",
    "@njit\n",
    "def rhs(X, t, theta):\n",
    "    # unpack parameters\n",
    "    x, y = X\n",
    "    alpha, beta, gamma, delta, xt0, yt0 = theta\n",
    "    # equations\n",
    "    dx_dt = alpha * x - beta * x * y\n",
    "    dy_dt = -gamma * y + delta * x * y\n",
    "    return [dx_dt, dy_dt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel for the model and make sure the equations are working correctly, let's run the model once with reasonable values for $\\theta$ and plot the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model function\n",
    "def plot_model(\n",
    "    ax,\n",
    "    x_y,\n",
    "    time=np.arange(1900, 1921, 0.01),\n",
    "    alpha=1,\n",
    "    lw=3,\n",
    "    title=\"Hudson's Bay Company Data and\\nExample Model Run\",\n",
    "):\n",
    "    ax.plot(time, x_y[:, 1], color=\"b\", alpha=alpha, lw=lw, label=\"Lynx (Model)\")\n",
    "    ax.plot(time, x_y[:, 0], color=\"g\", alpha=alpha, lw=lw, label=\"Hare (Model)\")\n",
    "    ax.legend(fontsize=14, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note theta = alpha, beta, gamma, delta, xt0, yt0\n",
    "theta = np.array([0.52, 0.026, 0.84, 0.026, 34.0, 5.9])\n",
    "time = np.arange(1900, 1921, 0.01)\n",
    "\n",
    "# call Scipy's odeint function\n",
    "x_y = odeint(func=rhs, y0=theta[-2:], t=time, args=(theta,))\n",
    "\n",
    "# plot\n",
    "_, ax = plt.subplots(figsize=(12, 4))\n",
    "plot_data(ax, lw=0)\n",
    "plot_model(ax, x_y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the `odeint` function is working as expected.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can solve the ODE using least squares.  Make a function that calculates the residual error.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that calculates residuals based on a given theta\n",
    "def ode_model_resid(theta):\n",
    "    return (\n",
    "        data[[\"hare\", \"lynx\"]] - odeint(func=rhs, y0=theta[-2:], t=data.year, args=(theta,))\n",
    "    ).values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed the residual error function to the Scipy `least_squares` solver.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate least squares using the Scipy solver\n",
    "results = least_squares(ode_model_resid, x0=theta)\n",
    "\n",
    "# put the results in a dataframe for presentation and convenience\n",
    "df = pd.DataFrame()\n",
    "parameter_names = [\"alpha\", \"beta\", \"gamma\", \"delta\", \"h0\", \"l0\"]\n",
    "df[\"Parameter\"] = parameter_names\n",
    "df[\"Least Squares Solution\"] = results.x\n",
    "df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(1900, 1921, 0.01)\n",
    "theta = results.x\n",
    "x_y = odeint(func=rhs, y0=theta[-2:], t=time, args=(theta,))\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "plot_data(ax, lw=0)\n",
    "plot_model(ax, x_y, title=\"Least Squares Solution\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks right.  If we didn't care about uncertainty, then we would be done.  But we do care about uncertainty, so let's move on to Bayesian inference.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyMC Model Specification for Gradient-Free Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like other Numpy or Scipy-based functions, the `scipy.integrate.odeint` function cannot be used directly in a PyMC model because PyMC needs to know the variable input and output types to compile.  Therefore, we use a Pytensor wrapper to give the variable types to PyMC.  Then the function can be used in PyMC in conjunction with gradient-free samplers.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Python Function to a Pytensor Operator using @as_op decorator\n",
    "We tell PyMC the input variable types and the output variable types using the `@as_op` decorator.  `odeint` returns Numpy arrays, but we tell PyMC that they are Pytensor double float tensors for this purpose.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decorator with input and output types a Pytensor double float tensors\n",
    "@as_op(itypes=[pt.dvector], otypes=[pt.dmatrix])\n",
    "def pytensor_forward_model_matrix(theta):\n",
    "    return odeint(func=rhs, y0=theta[-2:], t=data.year, args=(theta,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyMC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can specify the PyMC model using the ode solver!  For priors, we will use the results from the least squares calculation (`results.x`) to assign priors that start in the right range.  These are empirically derived weakly informative priors.  We also make them positive-only for this problem.      \n",
    "\n",
    "We will use a normal likelihood on untransformed data (i.e., not log transformed) to best fit the peaks of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = results.x  # least squares solution used to inform the priors\n",
    "with pm.Model() as model:\n",
    "    # Priors\n",
    "    alpha = pm.TruncatedNormal(\"alpha\", mu=theta[0], sigma=0.1, lower=0, initval=theta[0])\n",
    "    beta = pm.TruncatedNormal(\"beta\", mu=theta[1], sigma=0.01, lower=0, initval=theta[1])\n",
    "    gamma = pm.TruncatedNormal(\"gamma\", mu=theta[2], sigma=0.1, lower=0, initval=theta[2])\n",
    "    delta = pm.TruncatedNormal(\"delta\", mu=theta[3], sigma=0.01, lower=0, initval=theta[3])\n",
    "    xt0 = pm.TruncatedNormal(\"xto\", mu=theta[4], sigma=1, lower=0, initval=theta[4])\n",
    "    yt0 = pm.TruncatedNormal(\"yto\", mu=theta[5], sigma=1, lower=0, initval=theta[5])\n",
    "    sigma = pm.HalfNormal(\"sigma\", 1)\n",
    "\n",
    "    # Ode solution function\n",
    "    ode_solution = pytensor_forward_model_matrix(\n",
    "        pm.math.stack([alpha, beta, gamma, delta, xt0, yt0])\n",
    "    )\n",
    "\n",
    "    # Likelihood\n",
    "    pm.Normal(\"Y_obs\", mu=ode_solution, sigma=sigma, observed=data[[\"hare\", \"lynx\"]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Functions\n",
    "A couple of plotting functions that we will reuse below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_trace(ax, trace_df, row_idx, lw=1, alpha=0.2):\n",
    "    cols = [\"alpha\", \"beta\", \"gamma\", \"delta\", \"xto\", \"yto\"]\n",
    "    row = trace_df.iloc[row_idx, :][cols].values\n",
    "\n",
    "    # alpha, beta, gamma, delta, Xt0, Yt0\n",
    "    time = np.arange(1900, 1921, 0.01)\n",
    "    theta = row\n",
    "    x_y = odeint(func=rhs, y0=theta[-2:], t=time, args=(theta,))\n",
    "    plot_model(ax, x_y, time=time, lw=lw, alpha=alpha);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inference(\n",
    "    ax,\n",
    "    trace,\n",
    "    num_samples=25,\n",
    "    title=\"Hudson's Bay Company Data and\\nInference Model Runs\",\n",
    "    plot_model_kwargs=dict(lw=1, alpha=0.2),\n",
    "):\n",
    "    trace_df = az.extract(trace, num_samples=num_samples).to_dataframe()\n",
    "    plot_data(ax, lw=0)\n",
    "    for row_idx in range(num_samples):\n",
    "        plot_model_trace(ax, trace_df, row_idx, **plot_model_kwargs)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles[:2], labels[:2], loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    ax.set_title(title, fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Gradient-Free Sampler Options\n",
    "Having good gradient free samplers can open up the models that can be fit within PyMC.  There are five options for gradient-free samplers in PyMC that are applicable to this problem: \n",
    "* `Slice` - the default gradient-free sampler\n",
    "* `DEMetropolisZ` - a differential evolution Metropolis sampler that uses the past to inform sampling jumps\n",
    "* `DEMetropolis` - a differential evolution Metropolis sampler\n",
    "* `Metropolis` - the vanilla Metropolis sampler\n",
    "* `SMC` - Sequential Monte Carlo  \n",
    "\n",
    "Let's give them a shot.\n",
    "\n",
    "A few notes on running these inferences.  For each sampler, the number of tuning steps and draws have been reduced to run the inference in a reasonable amount of time (on the order of minutes).  This is not a sufficient number of draws to get a good inferences, in some cases, but it works for demonstration purposes.  In addition, multicore processing was not working for the Pytensor op function on all machines, so inference is performed on one core.         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable list to give to the sample step parameter\n",
    "vars_list = list(model.values_to_rvs.keys())[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the sampler\n",
    "sampler = \"Slice Sampler\"\n",
    "tune = draws = 2000\n",
    "\n",
    "# Inference!\n",
    "with model:\n",
    "    trace_slice = pm.sample(step=[pm.Slice(vars_list)], tune=tune, draws=draws)\n",
    "trace = trace_slice\n",
    "az.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, kind=\"rank_bars\")\n",
    "plt.suptitle(f\"Trace Plot {sampler}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "plot_inference(ax, trace, title=f\"Data and Inference Model Runs\\n{sampler} Sampler\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**  \n",
    "The Slice sampler was slow and resulted in a low effective sample size.  Despite this, the results are starting to look reasonable!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DE MetropolisZ Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = \"DEMetropolisZ\"\n",
    "tune = draws = 5000\n",
    "with model:\n",
    "    trace_DEMZ = pm.sample(step=[pm.DEMetropolisZ(vars_list)], tune=tune, draws=draws)\n",
    "trace = trace_DEMZ\n",
    "az.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, kind=\"rank_bars\")\n",
    "plt.suptitle(f\"Trace Plot {sampler}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "plot_inference(ax, trace, title=f\"Data and Inference\\n{sampler} Sampler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**  \n",
    "DEMetropolisZ sampled much quicker than the Slice sampler and therefore had a higher ESS per minute spent sampling.  The parameter estimates are similar.  A \"final\" inference would still need to beef up the number of samples.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEMetropolis Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these experiments, DEMetropolis sampler was not accepting `tune` and requiring `chains` to be at least 8. We set draws at 5000, lower number like 3000 produce bad mixing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = \"DEMetropolis\"\n",
    "chains = 8\n",
    "draws = 6000\n",
    "with model:\n",
    "    trace_DEM = pm.sample(step=[pm.DEMetropolis(vars_list)], draws=draws, chains=chains)\n",
    "trace = trace_DEM\n",
    "az.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, kind=\"rank_bars\")\n",
    "plt.suptitle(f\"Trace Plot {sampler}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "plot_inference(ax, trace, title=f\"Data and Inference Model Runs\\n{sampler} Sampler\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**  \n",
    "KDEs looks too wiggly, but ESS is high R-hat is good and rank_plots also look good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metropolis Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = \"Metropolis\"\n",
    "tune = draws = 5000\n",
    "with model:\n",
    "    trace_M = pm.sample(step=[pm.Metropolis(vars_list)], tune=tune, draws=draws)\n",
    "trace = trace_M\n",
    "az.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, kind=\"rank_bars\")\n",
    "plt.suptitle(f\"Trace Plot {sampler}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "plot_inference(ax, trace, title=f\"Data and Inference Model Runs\\n{sampler} Sampler\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**  \n",
    "The old-school Metropolis sampler is less reliable and slower than the DEMetroplis samplers.  Not recommended.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMC Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sequential Monte Carlo (SMC) sampler can be used to sample a regular Bayesian model or to run model without a likelihood (Aproximate Bayesian Computation). Let's try first with a regular model,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMC with a Likelihood Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = \"SMC with Likelihood\"\n",
    "draws = 2000\n",
    "with model:\n",
    "    trace_SMC_like = pm.sample_smc(draws,chains=4)\n",
    "trace = trace_SMC_like\n",
    "az.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.sample_stats._t_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, kind=\"rank_bars\")\n",
    "plt.suptitle(f\"Trace Plot {sampler}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "plot_inference(ax, trace, title=f\"Data and Inference Model Runs\\n{sampler} Sampler\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**  \n",
    "At this number of samples and tuning scheme, the SMC algorithm results in wider uncertainty bounds compared with the other samplers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMC Using `pm.Simulator` Epsilon=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As outlined in the SMC tutorial on PyMC.io, the SMC sampler can be used for Aproximate Bayesian Computation, i.e. we can use a `pm.Simulator` instead of a explicit likelihood.  Here is a rewrite of the PyMC - odeint model for SMC-ABC.\n",
    "\n",
    "The simulator function needs to have the correct signature (e.g., accept an rng argument first).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulator function based on the signature rng, parameters, size.\n",
    "def simulator_forward_model(rng, alpha, beta, gamma, delta, xt0, yt0, sigma, size=None):\n",
    "    theta = alpha, beta, gamma, delta, xt0, yt0\n",
    "    mu = odeint(func=rhs, y0=theta[-2:], t=data.year, args=(theta,))\n",
    "    return rng.normal(mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the model with the simulator function. Instead of a explicit likelihood function, the simulator uses distance metric (defaults to `gaussian`) between the simulated and observed values. When using a simulator we also need to specify epsilon, that is a tolerance value for the discrepancy between simulated and observed values. If epsilon is too low, SMC will not be able to move away from the initial values or a few values. We can easily see this with `az.plot_trace`. If epsilon is too high, the posterior will virtually be the prior. So"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Specify prior distributions for model parameters\n",
    "    alpha = pm.TruncatedNormal(\"alpha\", mu=theta[0], sigma=0.1, lower=0, initval=theta[0])\n",
    "    beta = pm.TruncatedNormal(\"beta\", mu=theta[1], sigma=0.01, lower=0, initval=theta[1])\n",
    "    gamma = pm.TruncatedNormal(\"gamma\", mu=theta[2], sigma=0.1, lower=0, initval=theta[2])\n",
    "    delta = pm.TruncatedNormal(\"delta\", mu=theta[3], sigma=0.01, lower=0, initval=theta[3])\n",
    "    xt0 = pm.TruncatedNormal(\"xto\", mu=theta[4], sigma=1, lower=0, initval=theta[4])\n",
    "    yt0 = pm.TruncatedNormal(\"yto\", mu=theta[5], sigma=1, lower=0, initval=theta[5])\n",
    "    sigma = pm.HalfNormal(\"sigma\", 10)\n",
    "\n",
    "    # ode_solution\n",
    "    pm.Simulator(\n",
    "        \"Y_obs\",\n",
    "        simulator_forward_model,\n",
    "        params=(alpha, beta, gamma, delta, xt0, yt0, sigma),\n",
    "        epsilon=1,\n",
    "        observed=data[[\"hare\", \"lynx\"]].values,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference.  Note the `progressbar` was throwing an error so it is turned off.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = \"SMC_epsilon=1\"\n",
    "draws = 2000\n",
    "with model:\n",
    "    trace_SMC_e1 = pm.sample_smc(draws=draws, progressbar=False)\n",
    "trace = trace_SMC_e1\n",
    "az.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, kind=\"rank_bars\")\n",
    "plt.suptitle(f\"Trace Plot {sampler}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "plot_inference(ax, trace, title=f\"Data and Inference Model Runs\\n{sampler} Sampler\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**  \n",
    "We can see that if epsilon is too low `plot_trace` will clearly show it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMC with Epsilon = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Specify prior distributions for model parameters\n",
    "    alpha = pm.TruncatedNormal(\"alpha\", mu=theta[0], sigma=0.1, lower=0, initval=theta[0])\n",
    "    beta = pm.TruncatedNormal(\"beta\", mu=theta[1], sigma=0.01, lower=0, initval=theta[1])\n",
    "    gamma = pm.TruncatedNormal(\"gamma\", mu=theta[2], sigma=0.1, lower=0, initval=theta[2])\n",
    "    delta = pm.TruncatedNormal(\"delta\", mu=theta[3], sigma=0.01, lower=0, initval=theta[3])\n",
    "    xt0 = pm.TruncatedNormal(\"xto\", mu=theta[4], sigma=1, lower=0, initval=theta[4])\n",
    "    yt0 = pm.TruncatedNormal(\"yto\", mu=theta[5], sigma=1, lower=0, initval=theta[5])\n",
    "    sigma = pm.HalfNormal(\"sigma\", 10)\n",
    "\n",
    "    # ode_solution\n",
    "    pm.Simulator(\n",
    "        \"Y_obs\",\n",
    "        simulator_forward_model,\n",
    "        params=(alpha, beta, gamma, delta, xt0, yt0, sigma),\n",
    "        epsilon=10,\n",
    "        observed=data[[\"hare\", \"lynx\"]].values,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = \"SMC epsilon=10\"\n",
    "draws = 2000\n",
    "with model:\n",
    "    trace_SMC_e10 = pm.sample_smc(draws=draws)\n",
    "trace = trace_SMC_e10\n",
    "az.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, kind=\"rank_bars\")\n",
    "plt.suptitle(f\"Trace Plot {sampler}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "plot_inference(ax, trace, title=f\"Data and Inference Model Runs\\n{sampler} Sampler\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**  \n",
    "Now that we set a larger value for epsilon we can see that the SMC sampler (plus simulator) provides good results. Choosing a value for epsilon will always involve some trial and error. So, what to do in practice? As epsilon is the scale of the distance function. If you don't have any idea of how much error do you expected to get between simulated and observed values then a rule of thumb for picking an initial guess for epsilon is to use a number smaller than the standard deviation of the observed data, how much smaller maybe one order of magnitude or so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Correlations\n",
    "As an aside, it is worth pointing out that the posterior parameter space is a difficult geometry for sampling.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_pair(trace_DEM, figsize=(8, 6), scatter_kwargs=dict(alpha=0.01), marginals=True)\n",
    "plt.suptitle(\"Pair Plot Showing Posterior Correlations\", size=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major observation here is that the posterior shape is pretty difficult for a sampler to handle, with positive correlations, negative correlations, crecent-shapes, and large variations in scale.  This contributes to the slow sampling (in addition to the computational overhead in solving the ODE thousands of times).  This is also fun to look at for understanding how the model parameters impact each other.       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference with Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUTS, the PyMC default sampler can only be used if gradients are supplied to the sampler.  In this section, we will solve the system of ODEs within PyMC in two different ways that supply the sampler with gradients.  The first is the built-in `pymc.ode.DifferentialEquation` solver, and the second is to forward simulate using `pytensor.scan`, which allows looping.  Note that there may be other better and faster ways to perform Bayesian inference with ODEs using gradients, such as the [sunode](https://sunode.readthedocs.io/en/latest/index.html) project, and [diffrax](https://www.pymc-labs.io/blog-posts/jax-functions-in-pymc-3-quick-examples/), which relies on JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyMC ODE Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pymc.ode` uses `scipy.odeint` under the hood to estimate a solution and then estimate the gradient through finite differences. \n",
    "\n",
    "The `pymc.ode` API is similar to `scipy.odeint`.  The right-hand-side equations are put in a function and written as if `y` and `p` are vectors, as follows.  (Even when your model has one state and/or one parameter, you should explicitly write `y[0]` and/or `p[0]`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhs_pymcode(y, t, p):\n",
    "    dX_dt = p[0] * y[0] - p[1] * y[0] * y[1]\n",
    "    dY_dt = -p[2] * y[1] + p[3] * y[0] * y[1]\n",
    "    return [dX_dt, dY_dt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DifferentialEquation` takes as arguments:\n",
    "\n",
    "* `func`: A function specifying the differential equation (i.e. $f(\\mathbf{y},t,\\mathbf{p})$),\n",
    "* `times`: An array of times at which data was observed,\n",
    "* `n_states`: The dimension of $f(\\mathbf{y},t,\\mathbf{p})$ (number of output parameters),\n",
    "* `n_theta`: The dimension of $\\mathbf{p}$ (number of input parameters),\n",
    "* `t0`: Optional time to which the initial condition belongs,  \n",
    "\n",
    "as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode_model = DifferentialEquation(\n",
    "    func=rhs_pymcode, times=data.year.values, n_states=2, n_theta=4, t0=data.year.values[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the ODE is specified, we can use it in our PyMC model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference with NUTS\n",
    "`pymc.ode` is quite slow, so for demonstration purposes, we will only draw a few samples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Priors\n",
    "    alpha = pm.TruncatedNormal(\"alpha\", mu=theta[0], sigma=0.1, lower=0, initval=theta[0])\n",
    "    beta = pm.TruncatedNormal(\"beta\", mu=theta[1], sigma=0.01, lower=0, initval=theta[1])\n",
    "    gamma = pm.TruncatedNormal(\"gamma\", mu=theta[2], sigma=0.1, lower=0, initval=theta[2])\n",
    "    delta = pm.TruncatedNormal(\"delta\", mu=theta[3], sigma=0.01, lower=0, initval=theta[3])\n",
    "    xt0 = pm.TruncatedNormal(\"xto\", mu=theta[4], sigma=1, lower=0, initval=theta[4])\n",
    "    yt0 = pm.TruncatedNormal(\"yto\", mu=theta[5], sigma=1, lower=0, initval=theta[5])\n",
    "    sigma = pm.HalfNormal(\"sigma\", 10)\n",
    "\n",
    "    # ode_solution\n",
    "    ode_solution = ode_model(y0=[xt0, yt0], theta=[alpha, beta, gamma, delta])\n",
    "\n",
    "    # Likelihood\n",
    "    pm.Normal(\"Y_obs\", mu=ode_solution, sigma=sigma, observed=data[[\"hare\", \"lynx\"]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = \"NUTS PyMC ODE\"\n",
    "tune = draws = 15\n",
    "with model:\n",
    "    trace_pymc_ode = pm.sample(tune=tune, draws=draws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = trace_pymc_ode\n",
    "az.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, kind=\"rank_bars\")\n",
    "plt.suptitle(f\"Trace Plot {sampler}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(12, 4))\n",
    "plot_inference(ax, trace, title=f\"Data and Inference Model Runs\\n{sampler} Sampler\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**  \n",
    "NUTS is starting to find to the correct posterior, but would need a whole lot more time to make a good inference.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate with Pytensor Scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can write the system of ODEs as a forward simulation solver within PyMC.  The way to write for-loops in PyMC is with `pytensor.scan.`  Gradients are then supplied to the sampler via autodifferentiation.    \n",
    "\n",
    "First, we should test that the time steps are sufficiently small to get a reasonable estimate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Time Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that accepts different numbers of time steps for testing.  The function also demonstrates how `pytensor.scan` is used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lotka-Volterra forward simulation model using scan\n",
    "def lv_scan_simulation_model(theta, steps_year=100, years=21):\n",
    "    # variables to control time steps\n",
    "    n_steps = years * steps_year\n",
    "    dt = 1 / steps_year\n",
    "\n",
    "    # PyMC model\n",
    "    with pm.Model() as model:\n",
    "        # Priors (these are static for testing)\n",
    "        alpha = theta[0]\n",
    "        beta = theta[1]\n",
    "        gamma = theta[2]\n",
    "        delta = theta[3]\n",
    "        xt0 = theta[4]\n",
    "        yt0 = theta[5]\n",
    "\n",
    "        # Lotka-Volterra calculation function\n",
    "        ## Similar to the right-hand-side functions used earlier\n",
    "        ## but with dt applied to the equations\n",
    "        def ode_update_function(x, y, alpha, beta, gamma, delta):\n",
    "            x_new = x + (alpha * x - beta * x * y) * dt\n",
    "            y_new = y + (-gamma * y + delta * x * y) * dt\n",
    "            return x_new, y_new\n",
    "\n",
    "        # Pytensor scan looping function\n",
    "        ## The function argument names are not intuitive in this context!\n",
    "        result, updates = pytensor.scan(\n",
    "            fn=ode_update_function,  # function\n",
    "            outputs_info=[xt0, yt0],  # initial conditions\n",
    "            non_sequences=[alpha, beta, gamma, delta],  # parameters\n",
    "            n_steps=n_steps,  # number of loops\n",
    "        )\n",
    "\n",
    "        # Put the results together and track the result\n",
    "        pm.Deterministic(\"result\", pm.math.stack([result[0], result[1]], axis=1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pickle\n",
    "import pytensor\n",
    "import os\n",
    "from IPython.display import display as print\n",
    "\n",
    "import core_lib\n",
    "import importlib\n",
    "importlib.reload(core_lib)\n",
    "\n",
    "from core_lib import plot_dataset,r2_loss,get_predict_ks, get_dcdt_func_for_sunode\n",
    "from core_lib import get_predict_starts\n",
    "from core_lib import get_model, get_model2, distance_func, MY_EPSILON\n",
    "from core_lib import MyDataset\n",
    "\n",
    "\n",
    "db_csv_path = \"dataset/data.csv\"\n",
    "# 假设都是一级动力学\n",
    "k_kinetics = np.repeat(1, 11).astype(np.uint8)\n",
    "# k_kinetics = np.array([0,0,0,0,1,1,0,0,1,1,0]).astype(np.uint8)\n",
    "ks = np.array([0.00071942, 0.00269696, 0.00498945, 0.00444931, 0.00571299, 0.00801272, 0.00131931, 0.00319959, 0.00415571, 0.00228432, 0.00177611])\n",
    "#  =======================================================\n",
    "import copy\n",
    "\n",
    "# t_eval = np.linspace(0, 150, 100)\n",
    "t_eval = np.array([0.5, 48, 96, 144])\n",
    "\n",
    "# 初始化原始数据\n",
    "dataset_ori = core_lib.MyDataset(db_csv_path)\n",
    "\n",
    "# 初始化正确的模拟数据\n",
    "dataset_sim = core_lib.MyDataset(db_csv_path)\n",
    "_cct_names = dataset_sim.get_cct_names()\n",
    "_c0 = dataset_sim.df[_cct_names].iloc[0].values\n",
    "dataset_sim.set_as_sim_dataset(t_eval, _c0, t0=0.5, args=(ks, k_kinetics))\n",
    "\n",
    "\n",
    "\n",
    "# 由模拟数据，生成fake字段\n",
    "dataset_fake = copy.deepcopy(dataset_sim)\n",
    "fake_colums = dataset_fake.get_fake_cct_names()\n",
    "dataset_fake.set_random(c_names=fake_colums)\n",
    "\n",
    "\n",
    "cct_names, error_names = dataset_fake.get_var_col_names()\n",
    "c0 = dataset_fake.df[cct_names].iloc[0].values\n",
    "plot_dataset(dataset_fake, dataset_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lotka-Volterra forward simulation model using scan\n",
    "importlib.reload(core_lib)\n",
    "\n",
    "def xj_scan_simulation_model( steps_year=100, years=21):\n",
    "    # variables to control time steps\n",
    "    n_steps = years * steps_year\n",
    "\n",
    "    # PyMC model\n",
    "    with pm.Model() as model:\n",
    "        from core_lib import get_dcdts_for_scipy_odeint\n",
    "        # pm.Simulator(\"CCT_Obs\", core_lib.simulator_forward_model, params=(t_eval, c0, parames, k_kinetics, sigma),distance=distance, epsilon=epsilon, observed=ccts)\n",
    "        \n",
    "        # Pytensor scan looping function\n",
    "        ## The function argument names are not intuitive in this context!\n",
    "        result, updates = pytensor.scan(\n",
    "            fn=get_dcdts_for_scipy_odeint(),  # function\n",
    "            outputs_info=c0,  # initial conditions\n",
    "            non_sequences=[2, ks, k_kinetics] ,  # parameters\n",
    "            n_steps=n_steps,  # number of loops\n",
    "        )\n",
    "\n",
    "        # Put the results together and track the result\n",
    "        pm.Deterministic(\"result\", pm.math.stack([result[0], result[1]], axis=1))\n",
    "\n",
    "    return model\n",
    "xj_scan_simulation_model(steps_year=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the simulation for various time steps and plot the results.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "steps_years = [12, 100, 1000, 10000]\n",
    "for steps_year in steps_years:\n",
    "    time = np.arange(1900, 1921, 1 / steps_year)\n",
    "    model = lv_scan_simulation_model(theta, steps_year=steps_year)\n",
    "    with model:\n",
    "        prior = pm.sample_prior_predictive(1)\n",
    "    ax.plot(time, prior.prior.result[0][0].values, label=str(steps_year) + \" steps/year\")\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "ax.set_title(\"Lotka-Volterra Forward Simulation Model with different step sizes\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the lower resolution simulations are less accurate over time.  Based on this check, 100 time steps per year is sufficiently accurate.  12 steps per year has too much \"numerical diffusion\" over 20 years of simulation.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Using NUTs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are OK with 100 time steps per year, we write the model with indexing to align the data with the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lv_scan_inference_model(theta, steps_year=100, years=21):\n",
    "    # variables to control time steps\n",
    "    n_steps = years * steps_year\n",
    "    dt = 1 / steps_year\n",
    "\n",
    "    # variables to control indexing to get annual values\n",
    "    segment = [True] + [False] * (steps_year - 1)\n",
    "    boolist_idxs = []\n",
    "    for _ in range(years):\n",
    "        boolist_idxs += segment\n",
    "\n",
    "    # PyMC model\n",
    "    with pm.Model() as model:\n",
    "        # Priors\n",
    "        alpha = pm.TruncatedNormal(\"alpha\", mu=theta[0], sigma=0.1, lower=0, initval=theta[0])\n",
    "        beta = pm.TruncatedNormal(\"beta\", mu=theta[1], sigma=0.01, lower=0, initval=theta[1])\n",
    "        gamma = pm.TruncatedNormal(\"gamma\", mu=theta[2], sigma=0.1, lower=0, initval=theta[2])\n",
    "        delta = pm.TruncatedNormal(\"delta\", mu=theta[3], sigma=0.01, lower=0, initval=theta[3])\n",
    "        xt0 = pm.TruncatedNormal(\"xto\", mu=theta[4], sigma=1, lower=0, initval=theta[4])\n",
    "        yt0 = pm.TruncatedNormal(\"yto\", mu=theta[5], sigma=1, lower=0, initval=theta[5])\n",
    "        sigma = pm.HalfNormal(\"sigma\", 10)\n",
    "\n",
    "        # Lotka-Volterra calculation function\n",
    "        def ode_update_function(x, y, alpha, beta, gamma, delta):\n",
    "            x_new = x + (alpha * x - beta * x * y) * dt\n",
    "            y_new = y + (-gamma * y + delta * x * y) * dt\n",
    "            return x_new, y_new\n",
    "\n",
    "        # Pytensor scan is a looping function\n",
    "        result, updates = pytensor.scan(\n",
    "            fn=ode_update_function,  # function\n",
    "            outputs_info=[xt0, yt0],  # initial conditions\n",
    "            non_sequences=[alpha, beta, gamma, delta],  # parameters\n",
    "            n_steps=n_steps,\n",
    "        )  # number of loops\n",
    "\n",
    "        # Put the results together\n",
    "        final_result = pm.math.stack([result[0], result[1]], axis=1)\n",
    "        # Filter the results down to annual values\n",
    "        annual_value = final_result[np.array(boolist_idxs), :]\n",
    "\n",
    "        # Likelihood function\n",
    "        pm.Normal(\"Y_obs\", mu=annual_value, sigma=sigma, observed=data[[\"hare\", \"lynx\"]].values)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also quite slow, so we will just pull a few samples for demonstration purposes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_year = 100\n",
    "model = lv_scan_inference_model(theta, steps_year=steps_year)\n",
    "sampler = \"NUTS Pytensor Scan\"\n",
    "tune = draws = 50\n",
    "with model:\n",
    "    trace_scan = pm.sample(tune=tune, draws=draws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = trace_scan\n",
    "az.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, kind=\"rank_bars\")\n",
    "plt.suptitle(f\"Trace Plot {sampler}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(1900, 1921, 0.01)\n",
    "odeint(func=rhs, y0=theta[-2:], t=time, args=(theta,)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(12, 4))\n",
    "plot_inference(ax, trace, title=f\"Data and Inference Model Runs\\n{sampler} Sampler\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**  \n",
    "The sampler is faster than the `pymc.ode` implementation, but still slower than scipy `odeint` combined with gradient-free inference methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare inference results among these different methods.  Recall that, in order to run this notebook in a reasonable amount of time, we have an insufficient number of samples for many inference methods.  For a fair comparison, we would need to bump up the number of samples and run the notebook for longer.  Regardless, let's take a look.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lists with variable for looping\n",
    "var_names = [str(s).split(\"_\")[0] for s in list(model.values_to_rvs.keys())[:-1]]\n",
    "# Make lists with model results and model names for plotting\n",
    "inference_results = [\n",
    "    trace_slice,\n",
    "    trace_DEMZ,\n",
    "    trace_DEM,\n",
    "    trace_M,\n",
    "    trace_SMC_like,\n",
    "    trace_SMC_e1,\n",
    "    trace_SMC_e10,\n",
    "    trace_pymc_ode,\n",
    "    trace_scan,\n",
    "]\n",
    "model_names = [\n",
    "    \"Slice Sampler\",\n",
    "    \"DEMetropolisZ\",\n",
    "    \"DEMetropolis\",\n",
    "    \"Metropolis\",\n",
    "    \"SMC with Likelihood\",\n",
    "    \"SMC e=1\",\n",
    "    \"SMC e=10\",\n",
    "    \"PyMC ODE NUTs\",\n",
    "    \"Pytensor Scan NUTs\",\n",
    "]\n",
    "\n",
    "# Loop through variable names\n",
    "for var_name in var_names:\n",
    "    axes = az.plot_forest(\n",
    "        inference_results,\n",
    "        model_names=model_names,\n",
    "        var_names=var_name,\n",
    "        kind=\"forestplot\",\n",
    "        legend=False,\n",
    "        combined=True,\n",
    "        figsize=(7, 3),\n",
    "    )\n",
    "    axes[0].set_title(f\"Marginal Probability: {var_name}\")\n",
    "    # Clean up ytick labels\n",
    "    ylabels = axes[0].get_yticklabels()\n",
    "    new_ylabels = []\n",
    "    for label in ylabels:\n",
    "        txt = label.get_text()\n",
    "        txt = txt.replace(\": \" + var_name, \"\")\n",
    "        label.set_text(txt)\n",
    "        new_ylabels.append(label)\n",
    "    axes[0].set_yticklabels(new_ylabels)\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**  \n",
    "If we ran the samplers for long enough to get good inferences, we would expect them to converge on the same posterior probability distributions. This is not necessarily true for Aproximate Bayssian Computation, unless we first ensure that the approximation too the likelihood is good enough. For instance SMCe=1 is providing a wrong result, we have been warning that this was most likely the case when we use `plot_trace` as a diagnostic. For SMC e=10, we see that posterior mean agrees with the other samplers, but the posterior is wider. This is expected with ABC methods. A smaller value of epsilon, maybe 5, should provide a posterior closer to the true one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Conclusions\n",
    "We performed Bayesian inference on a system of ODEs in 4 main ways: \n",
    "* Scipy `odeint` wrapped in a Pytensor `op` and sampled with non-gradient-based samplers (comparing 5 different samplers).  \n",
    "* Scipy `odeint` wrapped in a `pm.Simulator` function and sampled with a non-likelihood-based sequential Monte Carlo (SMC) sampler.  \n",
    "* PyMC `ode.DifferentialEquation` sampled with NUTs.  \n",
    "* Forward simulation using `pytensor.scan` and sampled with NUTs.  \n",
    "\n",
    "The \"winner\" for this problem was the Scipy `odeint` solver with a differential evolution (DE) Metropolis sampler and SMC (for a model with a Likelihood) provide good results with SMC being somewhat slower (but also better diagnostics). The improved efficiency of the NUTS sampler did not make up for the inefficiency in using the slow ODE solvers with gradients.  Both DEMetropolis and SMC enable the simplest workflow for a scientist with a working numeric model and the desire to perform Bayesian inference. Just wrapping the numeric model in a Pytensor op and plugging it into a PyMC model can get you a long way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Authors\n",
    "Organized and rewritten by [Greg Brunkhorst](https://github.com/gbrunkhorst)  from multiple legacy PyMC.io example notebooks by Sanmitra Ghosh, Demetri Pananos, and the PyMC Team ({ref}`ABC_introduction`).\n",
    "\n",
    "Osvaldo Martin added some clarification about SMC-ABC and  minor fixes in Mar, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    ":::{bibliography}\n",
    ":filter: docname in docnames\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark -n -u -v -iv -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{include} ../page_footer.md\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b6ee0dc654e6abf1449e02fb30366f6de03b4a07aea929f5498fe7470f90fafb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
